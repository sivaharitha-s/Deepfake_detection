{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M6pPkqT8Jxf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
      ],
      "metadata": {
        "id": "32NdFZV08TGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/content/styleGAN', exist_ok=True)"
      ],
      "metadata": {
        "id": "6ERhOl5Z8ZeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/StyleGAN.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/styleGAN')"
      ],
      "metadata": {
        "id": "hVpEqKDH8bhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/styleGAN/StyleGAN/fake'\n",
        "\n",
        "original_fake_paths = []\n",
        "\n",
        "for dirname, _, filenames in tqdm(os.walk(base_dir)):\n",
        "    for filename in filenames:\n",
        "        if filename.lower().endswith(('.jpg', '.png')):\n",
        "            original_fake_paths.append([os.path.join(dirname, filename), filename])\n",
        "\n",
        "print(f\"Total fake images found: {len(original_fake_paths)}\")"
      ],
      "metadata": {
        "id": "J5zQrJ9h8cic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/tmp/fake/'\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "fake_paths = [os.path.join(save_dir, filename) for _, filename in original_fake_paths]\n",
        "\n",
        "for path, filename in tqdm(original_fake_paths):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    cv2.imwrite(os.path.join(save_dir, filename), img)"
      ],
      "metadata": {
        "id": "iKguBa4t8hFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_fake_paths, test_fake_paths = train_test_split(fake_paths, test_size=5000, random_state=2019)\n",
        "\n",
        "fake_train_df = pd.DataFrame(train_fake_paths, columns=['filename'])\n",
        "fake_train_df['class'] = 'FAKE'\n",
        "\n",
        "fake_test_df = pd.DataFrame(test_fake_paths, columns=['filename'])\n",
        "fake_test_df['class'] = 'FAKE'\n",
        "\n",
        "print(\"Train dataframe shape:\", fake_train_df.shape)\n",
        "print(\"Test dataframe shape:\", fake_test_df.shape)"
      ],
      "metadata": {
        "id": "XtUuyuE38kDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/content/celebA', exist_ok=True)"
      ],
      "metadata": {
        "id": "XTVZe7G78l3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/CelebA.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/celebA')"
      ],
      "metadata": {
        "id": "UgxhI5Fo8mdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_dir = '/content/celebA/CelebA/real'\n",
        "real_save_dir = '/content/tmp/real/'\n",
        "\n",
        "if not os.path.exists(real_save_dir):\n",
        "    os.makedirs(real_save_dir)\n",
        "\n",
        "real_image_paths = []\n",
        "for filename in tqdm(os.listdir(real_dir)):\n",
        "    if filename.lower().endswith(('.jpg', '.png')):\n",
        "        path = os.path.join(real_dir, filename)\n",
        "        real_image_paths.append(path)\n",
        "\n",
        "\n",
        "        img = cv2.imread(path)\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        cv2.imwrite(os.path.join(real_save_dir, filename), img)\n",
        "\n",
        "real_df = pd.DataFrame(real_image_paths, columns=['filename'])\n",
        "real_df['class'] = 'REAL'"
      ],
      "metadata": {
        "id": "5e91o97J8psd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "real_train_df, real_test_df = train_test_split(real_df, test_size=5000, random_state=2019)\n",
        "\n",
        "print(\"Train dataframe shape:\", real_train_df.shape)\n",
        "print(\"Test dataframe shape:\", real_test_df.shape)"
      ],
      "metadata": {
        "id": "ZfewiyAQ8rXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.concat([real_train_df, fake_train_df]).reset_index(drop=True)\n",
        "test_df = pd.concat([real_test_df, fake_test_df]).reset_index(drop=True)\n",
        "\n",
        "print(\"Combined train dataframe shape:\", train_df.shape)\n",
        "print(\"Combined test dataframe shape:\", test_df.shape)"
      ],
      "metadata": {
        "id": "H00oFf1o8uOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, BatchNormalization, Activation,\n",
        "    AveragePooling2D, GlobalAveragePooling2D,\n",
        "    Dense, Concatenate, Dropout\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_gen = datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "9VHADsNA8w9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, BatchNormalization, Activation,\n",
        "    MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D,\n",
        "    Dense, Concatenate, Dropout, RandomFlip, RandomRotation, RandomZoom\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# ---- Dense Block ----\n",
        "def dense_block(x, num_layers, growth_rate):\n",
        "    \"\"\"Dense block with concatenated feature maps\"\"\"\n",
        "    for i in range(num_layers):\n",
        "        bn = BatchNormalization()(x)\n",
        "        relu = Activation('relu')(bn)\n",
        "        conv = Conv2D(4 * growth_rate, (1, 1), padding='same', use_bias=False)(relu)\n",
        "        bn = BatchNormalization()(conv)\n",
        "        relu = Activation('relu')(bn)\n",
        "        conv = Conv2D(growth_rate, (3, 3), padding='same', use_bias=False)(relu)\n",
        "        x = Concatenate()([x, conv])\n",
        "    return x\n",
        "\n",
        "# ---- Transition Layer ----\n",
        "def transition_layer(x, reduction):\n",
        "    \"\"\"Reduce feature map dimensions\"\"\"\n",
        "    bn = BatchNormalization()(x)\n",
        "    relu = Activation('relu')(bn)\n",
        "    num_filters = int(tf.keras.backend.int_shape(x)[-1] * reduction)\n",
        "    conv = Conv2D(num_filters, (1, 1), padding='same', use_bias=False)(relu)\n",
        "    avg_pool = AveragePooling2D((2, 2), strides=2)(conv)\n",
        "    return avg_pool\n",
        "\n",
        "def build_densenet121_scratch(input_shape=(224, 224, 3), num_classes=1,\n",
        "                               growth_rate=32, reduction=0.5, use_augmentation=True):\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    # Data augmentation\n",
        "    x = input_tensor\n",
        "    if use_augmentation:\n",
        "        x = RandomFlip(\"horizontal\")(x)\n",
        "        x = RandomRotation(0.05)(x)\n",
        "        x = RandomZoom(0.05)(x)\n",
        "\n",
        "    # Initial Conv + MaxPooling\n",
        "    x = Conv2D(64, (7, 7), strides=2, padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=2, padding='same')(x)\n",
        "\n",
        "    # Dense Blocks + Transition Layers\n",
        "    x = dense_block(x, 6, growth_rate)\n",
        "    x = transition_layer(x, reduction)\n",
        "    x = dense_block(x, 12, growth_rate)\n",
        "    x = transition_layer(x, reduction)\n",
        "    x = dense_block(x, 24, growth_rate)\n",
        "    x = transition_layer(x, reduction)\n",
        "    x = dense_block(x, 16, growth_rate)\n",
        "\n",
        "    # Final layers\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(num_classes, activation='sigmoid', name='predictions')(x)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=output, name='DenseNet121_Scratch')\n",
        "    return model\n",
        "\n",
        "# ---- Build model ----\n",
        "model = build_densenet121_scratch(input_shape=(224, 224, 3), num_classes=1, use_augmentation=True)\n",
        "\n",
        "# ---- Compile model ----\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy',\n",
        "             tf.keras.metrics.AUC(name='auc'),\n",
        "             tf.keras.metrics.Precision(name='precision'),\n",
        "             tf.keras.metrics.Recall(name='recall')]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "fyAgVJcC8x9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'model.h5',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_step1 = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    epochs=10,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "kpHb7Ot6K0Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading weights...\")\n",
        "model.load_weights('/content/drive/MyDrive/model (1).h5')\n",
        "print(\"Weights loaded successfully!\")"
      ],
      "metadata": {
        "id": "tqTJQTT583p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unfreezing all layers...\")\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "checkpoint_ft = ModelCheckpoint(\n",
        "    '/content/drive/MyDrive/model_finetuned.h5',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "print(\"Starting fine-tuning...\")\n",
        "history_step2 = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    steps_per_epoch=len(train_gen),\n",
        "    validation_steps=len(val_gen),\n",
        "    epochs=3,\n",
        "    callbacks=[checkpoint_ft]\n",
        ")\n",
        "\n",
        "print(\"Fine-tuning completed!\")"
      ],
      "metadata": {
        "id": "dyR_sR_NK4K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "def preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "image_path = '/content/drive/MyDrive/Screenshot 2025-10-13 213907.png'\n",
        "img = preprocess_image(image_path)\n",
        "pred = model.predict(img)[0][0]\n",
        "\n",
        "if pred > 0.5:\n",
        "    print(\"Prediction: REAL\")\n",
        "else:\n",
        "    print(\"Prediction: FAKE\")\n",
        "\n",
        "print(\"Prediction score:\", pred)"
      ],
      "metadata": {
        "id": "HQiehjGp89Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/model_finetuned.h5')\n",
        "print(\"âœ… Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "vEtnMJerIbMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_acc = model.evaluate(train_gen, steps=len(train_gen), verbose=1)\n",
        "print(f\"Train Accuracy: {train_acc*100:.2f}%\")\n",
        "print(f\"Train Loss: {train_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "1FJnBedgIhTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_gen, steps=len(test_gen), verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "d253mhlUImfU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}